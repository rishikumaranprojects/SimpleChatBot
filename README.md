# A simple Gradio app to chat with a local Ollama model
# Make sure you have Ollama running locally with the desired model.
# Install required packages: pip install gradio requests
# Run the app: python chatbot.py
# Access the app at http://127.0.0.1:7860
# Ensure Ollama is running: ollama serve
# Example Ollama command to run a model: ollama run tinydolphin:1.1b
# Adjust model name as needed.
# Note: This example assumes you have a model named "tinydolphin:1.1b" available locally.
# Adjust the model name in the code if necessary.
